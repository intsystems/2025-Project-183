{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Effective Training",
   "id": "732f1413c8a8de26"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T00:29:45.709999Z",
     "start_time": "2025-04-30T00:29:44.461790Z"
    }
   },
   "source": [
    " import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.models import MLP\n",
    "from src.models import estimate_train\n",
    "from src.utils import init_dataloader\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T00:29:45.735327Z",
     "start_time": "2025-04-30T00:29:45.712503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = init_dataloader(\n",
    "    dataset_name='MNIST',\n",
    "    transform=transform,\n",
    "    batch_size=64,\n",
    "    dataset_load_path='data/',\n",
    "    train_mode=True,\n",
    "    size=64 * (10000 // 64)\n",
    ")\n",
    "\n",
    "test_loader = init_dataloader(\n",
    "    dataset_name='MNIST',\n",
    "    transform=transform,\n",
    "    batch_size=64,\n",
    "    dataset_load_path='data/',\n",
    "    train_mode=False,\n",
    "    size=64 * (10000 // 64)\n",
    ")"
   ],
   "id": "38ca246d4825e16a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T00:43:54.794237Z",
     "start_time": "2025-04-30T00:29:45.799377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MLP(layers_num=2, hidden=256, input_channels=1, input_sizes=(28, 28), classes=10).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "estimate_train(model, criterion, train_loader, optimizer, delta=0.001, num_epochs=10, log=True)"
   ],
   "id": "aa63ae6a5ed20498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/156]: loss = 0.7626, delta = 1000000000.0000\n",
      "Batch [2/156]: loss = 0.4929, delta = 1422.2661\n",
      "Batch [3/156]: loss = 0.3658, delta = 269.0240\n",
      "Batch [4/156]: loss = 0.3602, delta = 7.1685\n",
      "Batch [5/156]: loss = 0.4044, delta = 8.9065\n",
      "Batch [6/156]: loss = 0.3767, delta = 11.1330\n",
      "Batch [7/156]: loss = 0.2739, delta = 1.3913\n",
      "Batch [8/156]: loss = 0.1711, delta = 4.0212\n",
      "Batch [9/156]: loss = 0.2007, delta = 9.4660\n",
      "Batch [10/156]: loss = 0.1924, delta = 7.9431\n",
      "Batch [11/156]: loss = 0.1707, delta = 20.1311\n",
      "Batch [12/156]: loss = 0.2704, delta = 0.4660\n",
      "Batch [13/156]: loss = 0.2685, delta = 0.3382\n",
      "Batch [14/156]: loss = 0.2802, delta = 2.6122\n",
      "Batch [15/156]: loss = 0.1615, delta = 0.0536\n",
      "Batch [16/156]: loss = 0.1839, delta = 0.2009\n",
      "Batch [17/156]: loss = 0.1281, delta = 0.9414\n",
      "Batch [18/156]: loss = 0.1681, delta = 0.3503\n",
      "Batch [19/156]: loss = 0.1684, delta = 0.0012\n",
      "Batch [20/156]: loss = 0.2672, delta = 0.0163\n",
      "Batch [21/156]: loss = 0.2061, delta = 0.0138\n",
      "Batch [22/156]: loss = 0.1349, delta = 0.0153\n",
      "Batch [23/156]: loss = 0.1058, delta = 0.1929\n",
      "Batch [24/156]: loss = 0.1462, delta = 0.1224\n",
      "Batch [25/156]: loss = 0.2703, delta = 0.0475\n",
      "Batch [26/156]: loss = 0.1777, delta = 0.0120\n",
      "Batch [27/156]: loss = 0.2319, delta = 0.0886\n",
      "Batch [28/156]: loss = 0.1661, delta = 0.0227\n",
      "Batch [29/156]: loss = 0.0930, delta = 1.1369\n",
      "Batch [30/156]: loss = 0.2153, delta = 0.0482\n",
      "Batch [31/156]: loss = 0.2510, delta = 0.1576\n",
      "Batch [32/156]: loss = 0.1796, delta = 0.0014\n",
      "Batch [33/156]: loss = 0.0863, delta = 0.0364\n",
      "Batch [34/156]: loss = 0.1837, delta = 1.4286\n",
      "Batch [35/156]: loss = 0.1836, delta = 0.0397\n",
      "Batch [36/156]: loss = 0.1252, delta = 0.0132\n",
      "Batch [37/156]: loss = 0.3250, delta = 0.5448\n",
      "Batch [38/156]: loss = 0.2462, delta = 0.0617\n",
      "Batch [39/156]: loss = 0.1458, delta = 0.1503\n",
      "Batch [40/156]: loss = 0.1608, delta = 0.1486\n",
      "Batch [41/156]: loss = 0.0776, delta = 0.0166\n",
      "Batch [42/156]: loss = 0.1461, delta = 0.1525\n",
      "Batch [43/156]: loss = 0.2761, delta = 0.0754\n",
      "Batch [44/156]: loss = 0.3893, delta = 0.0164\n",
      "Batch [45/156]: loss = 0.1877, delta = 0.0013\n",
      "Batch [46/156]: loss = 0.0574, delta = 0.0035\n",
      "Batch [47/156]: loss = 0.1075, delta = 0.0242\n",
      "Batch [48/156]: loss = 0.1164, delta = 0.0708\n",
      "Batch [49/156]: loss = 0.1924, delta = 0.0683\n",
      "Batch [50/156]: loss = 0.2334, delta = 0.0222\n",
      "Batch [51/156]: loss = 0.0650, delta = 0.0100\n",
      "Batch [52/156]: loss = 0.2369, delta = 0.0062\n",
      "Batch [53/156]: loss = 0.1102, delta = 0.0144\n",
      "Batch [54/156]: loss = 0.1366, delta = 0.0060\n",
      "Batch [55/156]: loss = 0.2815, delta = 0.0162\n",
      "Batch [56/156]: loss = 0.1797, delta = 0.0100\n",
      "Batch [57/156]: loss = 0.1018, delta = 0.0261\n",
      "Batch [58/156]: loss = 0.2000, delta = 0.0199\n",
      "Batch [59/156]: loss = 0.1926, delta = 0.0104\n",
      "Batch [60/156]: loss = 0.1383, delta = 0.0063\n",
      "Batch [61/156]: loss = 0.1337, delta = 0.0046\n",
      "Batch [62/156]: loss = 0.1813, delta = 0.0265\n",
      "Batch [63/156]: loss = 0.4255, delta = 0.0399\n",
      "Batch [64/156]: loss = 0.2794, delta = 0.0017\n",
      "Batch [65/156]: loss = 0.1707, delta = 0.0043\n",
      "Batch [66/156]: loss = 0.0495, delta = 0.0038\n",
      "Batch [67/156]: loss = 0.0906, delta = 0.0114\n",
      "Batch [68/156]: loss = 0.1767, delta = 0.0071\n",
      "Batch [69/156]: loss = 0.1499, delta = 0.0096\n",
      "Batch [70/156]: loss = 0.1291, delta = 0.0095\n",
      "Batch [71/156]: loss = 0.1862, delta = 0.0050\n",
      "Batch [72/156]: loss = 0.0735, delta = 0.0067\n",
      "Batch [73/156]: loss = 0.2137, delta = 0.0112\n",
      "Batch [74/156]: loss = 0.1529, delta = 0.0058\n",
      "Batch [75/156]: loss = 0.1711, delta = 0.0072\n",
      "Batch [76/156]: loss = 0.1072, delta = 0.0018\n",
      "Batch [77/156]: loss = 0.1175, delta = 0.0188\n",
      "Batch [78/156]: loss = 0.2275, delta = 0.0131\n",
      "Batch [79/156]: loss = 0.1754, delta = 0.0105\n",
      "Batch [80/156]: loss = 0.1007, delta = 0.0876\n",
      "Batch [81/156]: loss = 0.0717, delta = 0.0034\n",
      "Batch [82/156]: loss = 0.2083, delta = 0.0090\n",
      "Batch [83/156]: loss = 0.2347, delta = 0.0200\n",
      "Batch [84/156]: loss = 0.0965, delta = 0.0079\n",
      "Batch [85/156]: loss = 0.0775, delta = 0.0323\n",
      "Batch [86/156]: loss = 0.0773, delta = 0.0148\n",
      "Batch [87/156]: loss = 0.1911, delta = 0.0003\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
