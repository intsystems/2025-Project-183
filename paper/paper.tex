\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{a4wide}

\title{Neural Networks Loss Landscape Convergence in Different Low-Dimensional Spaces}
\author{Tem Nikitin, Nikita Kiselev, Vladislav Meshkov, Andrey Grabovoy}
\date{2025}
\begin{document}
\maketitle

\begin{abstract}
    Understanding how the loss landscape of neural networks evolves as the training set size increases is crucial for
    optimizing performance and ensuring reliable generalization. While it is well known that larger datasets can alter
    the shape of this high-dimensional landscape, the exact point at which additional data no longer brings substantial
    changes remains underexplored.

    In this paper, we examine neural network models and show that their loss landscapes begin to stabilize once the
    training set grows beyond a certain threshold, revealing a connection between dataset size and the geometry of the
    loss surface. To elucidate this phenomenon, we propose a method that projects the full parameter space onto a
    low-dimensional subspace derived from top eigenvectors (e.g., from the Hessian). Focusing on these principal directions
    preserves critical curvature information while providing a more interpretable view of how the loss surface in the
    vicinity of local minima behaves as more data become available. We further leverage targeted sampling strategies,
    applying Monte-Carlo estimation to capture the structure of this reduced loss landscape more precisely.

    We validate our insights through comprehensive experiments on image classification tasks, demonstrating that this
    low-dimensional analysis can reveal when the landscape effectively settles, and thus helps determine a minimum viable
    dataset size. Our findings shed light on the relationship between dataset scale and optimization geometry, and suggest
    practical strategies for balancing computational costs with the benefits of additional training data.
\end{abstract}
\paragraph{Keywords:} 
Neural networks, Loss landscape, Low-dimensional subspace, Hessian eigenvectors, Monte Carlo estimation, Dataset size threshold.

\section{Introduction}

\bibliographystyle{unsrt}
\bibliography{bibliography}
\end{document}
