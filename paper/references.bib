@article{kiselev2024unraveling,
  title   = {Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  author  = {Kiselev, Nikita and Grabovoy, Andrey},
  year    = {2024},
  journal = {arXiv preprint arXiv:2409.11995},
  note    = {Upper bounds via Hessian for fully connected neural networks.}
}

@misc{hessian-eigenthings,
  author  = {Noah Golmant and Zhewei Yao and Amir Gholami and Michael Mahoney and Joseph Gonzalez},
  title   = {pytorch-hessian-eigenthings: efficient PyTorch Hessian eigendecomposition},
  month   = oct,
  year    = 2018,
  version = {1.0},
  url     = {https://github.com/noahgolmant/pytorch-hessian-eigenthings}
}

@misc{meshkov2024convnets,
  title  = {ConvNets Landscape Convergence: Hessian-Based Analysis of Matricized Networks},
  author = {Meshkov, Vladislav and Kiselev, Nikita and Grabovoy, Andrey},
  year   = {2024},
  note   = {Upper bounds via Hessian for convolutional neural networks.},
  url    = {https://ieeexplore.ieee.org/document/10899113}
}

@article{sagun2018empirical,
  title   = {Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author  = {Sagun, Levent and Evci, Utku and Guney, Ugur and Dauphin, Yann and Bottou, Leon},
  year    = {2018},
  journal = {preprint arXiv:1706.04454},
  note    = {In overtrained neural networks the Hessian rank is often low; the active subspace may be significantly smaller.}
}

@article{li2018visualizing,
  title   = {Visualizing the Loss Landscape of Neural Nets},
  author  = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year    = {2018},
  journal = {preprint arXiv:1712.09913},
  note    = {Methods of visualizing loss function curvature and comparing different functions.}
}

@inproceedings{ghorbani2019investigation,
  title     = {An Investigation into Neural Net Optimization via Hessian Eigenvalue Density},
  author    = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  booktitle = {International Conference on Machine Learning},
  year      = {2019},
  note      = {Observed large isolated eigenvalues in the spectrum and a surprising concentration of the gradient in the corresponding eigenspaces.},
  url       = {https://proceedings.mlr.press/v97/ghorbani19b/ghorbani19b.pdf}
}

@article{wu2017towards,
  title   = {Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes},
  author  = {Wu, Lei and Zhu, Zhanxing and E, Weinan},
  year    = {2017},
  journal = {preprint arXiv:1706.10239},
  note    = {Characteristics of the loss landscape explain the good generalization capability.}
}

@article{hoffmann2022training,
  title   = {Training Compute-Optimal Large Language Models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  year    = {2022},
  journal = {preprint arXiv:2203.15556},
  note    = {Motivations behind overloaded models and the importance of optimal training size estimation.}
}

@misc{petersen2012matrix,
  title        = {The Matrix Cookbook},
  author       = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year         = {2012},
  howpublished = {\url{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}},
  note         = {A collection of facts about matrices and matters relating to them.}
}

@article{deng2012mnist,
  title   = {The mnist database of handwritten digit images for machine learning research},
  author  = {Deng, Li},
  year    = {2012},
  journal = {IEEE Signal Processing Magazine},
  volume  = {29},
  number  = {6},
  note    = {The database of handwritten digits.},
  url     = {https://api.semanticscholar.org/CorpusID:5280072}
}

@article{xiao2017fashion,
  title   = {Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author  = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year    = {2017},
  journal = {preprint arXiv:1708.07747},
  note    = {A dataset of Zalando's article images.}
}

@article{soydaner2020comparison,
  title   = {A comparison of optimization algorithms for deep learning},
  author  = {Soydaner, Derya},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  volume  = {34},
  number  = {13},
  pages   = {2052013},
  year    = {2020},
  month   = {April},
  issn    = {1793-6381},
  doi     = {10.1142/s0218001420520138},
  url     = {http://dx.doi.org/10.1142/S0218001420520138}
}

@inproceedings{yao2018hessian,
  title     = {Hessian-based Analysis of Large Batch Training and Robustness to Adversaries},
  author    = {Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W.},
  booktitle = {Advances in Neural Information Processing Systems},
  series    = {NeurIPS 2018},
  volume    = {32},
  year      = {2018},
  address   = {Montr√©al, Canada},
  note      = {arXiv:1802.08241v4 [cs.CV], 2 Dec 2018},
  url       = {https://arxiv.org/abs/1802.08241}
}

@article{golestaneh2024samples,
  title   = {How Many Samples Are Needed to Train a Deep Neural Network?},
  author  = {Golestaneh, Pegah and Taheri, Mahsa and Lederer, Johannes},
  journal = {arXiv preprint arXiv:2405.16696v1},
  year    = {2024},
  month   = {May},
  note    = {[math.ST]},
  url     = {https://arxiv.org/abs/2405.16696}
}

@article{bousquet2002stability,
  title   = {Stability and Generalization},
  author  = {Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal = {Journal of Machine Learning Research},
  volume  = {2},
  pages   = {499--526},
  year    = {2002},
  note    = {Submitted July 2001; Published March 2002}
}

@article{elisseeff2005stability,
  title   = {Stability of Randomized Learning Algorithms},
  author  = {Elisseeff, Andr{\'e} and Evgeniou, Theodoros and Pontil, Massimiliano},
  journal = {Journal of Machine Learning Research},
  volume  = {6},
  pages   = {55--79},
  year    = {2005},
  note    = {Submitted February 2004; Revised August 2004; Published January 2005}
}
