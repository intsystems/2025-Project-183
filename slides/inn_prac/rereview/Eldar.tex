\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\begin{document}

\pagestyle{empty}

\begin{center}
    \subsection*{Ответ на замечание рецензента Хузина Эльдара Руслановича}
\end{center}

\begin{quote}
    \textbf{Рецензент:}
    ``Сильной стороной работы считаю её потенциал для уменьшения порога входа начинающим специалистам --- т.е. быть может
    работа даст стимул к созданию небольших, но качественных датасетов...''
\end{quote}

\paragraph{Ответ.}
Полностью согласен: одной из моих мотиваций было именно показать, что крупномасштабные датасеты не всегда необходимы, и
что методика оценки ``точки насыщения'' позволяет сфокусироваться на сборе небольших, но содержательных выборок.

\begin{quote}
    \textbf{Рецензент:}
    ``Из вопросов, кажется, неприятным использование достаточно сложных архитектур, иначе говоря, при использовании хорошего
    математического аппарата для анализа, у нас имеется сложная структура на вход, влияние свойств которой могут плохо
    поддаваться учёту.''
\end{quote}

\paragraph{Ответ.}
Позволю себе пояснить выбор архитектур и масштаб исследования:
\begin{itemize}
    \item На этапах теоретической разработки и начальных экспериментов я сознательно использовал простую MLP-конфигурацию
          на MNIST/Fashion-MNIST, чтобы исключить влияние сложности входных данных и архитектурных деталей. Это дало возможность
          изолированно проверить поведение $\Delta_k$ и качество аппроксимаций, основанных на первых $d$ собственных векторах
          Гессиана.
    \item В будущей работе (раздел ``Планы и будущие работы'') я планирую расширить экспериментальную часть на современные
          свёрточные сети (ResNet, ConvNet). При этом сложность архитектуры и разнородность данных будут учитываться через 
          повторный расчёт спектра Гессиана в каждом слое или блоке, что позволит контролировать влияние дополнительных факторов.
    \item Таким образом, первоначально ``простой'' выбор сети обеспечил ясность и воспроизводимость, а последующее
          масштабирование на сложные архитектуры подтвердит общность и устойчивость метода.
\end{itemize}

\begin{quote}
    \textbf{Рецензент:}
    ``Данная работа ищет ответ на сопутствующий вопрос: так ли нам нужны большие датасеты?''
\end{quote}

\paragraph{Ответ.}
Да, именно так: главная цель исследования — предложить количественный критерий ``достаточного размера выборки'' $K$,
после которого дополнительная выборка даёт пренебрежимо малые изменения ландшафта потерь. Это позволяет делать обоснованные
решения о прекращении сбора данных и оптимизации ресурсов при обучении нейросетей.

\end{document}
